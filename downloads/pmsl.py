import os
import requests
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

DATE = os.environ.get("DATE")
RUN = os.environ.get("RUN")

os.makedirs("data/pmsl", exist_ok=True)

FIELD_REGEX = r":\s*PRMSL:mean sea level"
MAX_WORKERS = 6  # Anzahl paralleler Downloads (anpassbar)
RETRY_LIMIT = 3  # erneute Versuche falls Fehler


def fetch_pmsl(fh):
    fh_padded = f"{fh:03d}"

    base = f"https://noaa-nws-graphcastgfs-pds.s3.amazonaws.com/graphcastgfs.{DATE}/{RUN}/forecasts_13_levels"
    idx_url = f"{base}/graphcastgfs.t{RUN}z.pgrb2.0p25.f{fh_padded}.idx"
    grib_url = f"{base}/graphcastgfs.t{RUN}z.pgrb2.0p25.f{fh_padded}"

    out = f"data/pmsl/pmsl_{fh_padded}.grib2"

    for attempt in range(1, RETRY_LIMIT + 1):
        try:
            print(f"üì• [{fh_padded}] Index laden‚Ä¶")
            r = requests.get(idx_url, timeout=20)
            if r.status_code != 200:
                return f"‚ö†Ô∏è [{fh_padded}] idx fehlt ‚Üí √ºbersprungen"
            idx_data = r.text.splitlines()

            all_offsets = []
            t2m_offsets = []

            for line in idx_data:
                if ":" not in line:
                    continue
                parts = line.split(":")
                if len(parts) < 3 or not parts[1].isdigit():
                    continue

                offset = int(parts[1])
                all_offsets.append(offset)

                if re.search(FIELD_REGEX, line):
                    t2m_offsets.append(offset)

            if not t2m_offsets:
                return f"‚ö†Ô∏è [{fh_padded}] Kein t2m gefunden"

            head = requests.head(grib_url, timeout=10)
            filesize = int(head.headers.get("Content-Length", 0))

            ranges = []
            for start in t2m_offsets:
                nxt = [o for o in all_offsets if o > start]
                end = min(nxt) - 1 if nxt else filesize - 1
                ranges.append((start, end))

            print(f"‚Üí [{fh_padded}] {len(ranges)} Felder ‚Üí Download‚Ä¶")

            with open(out, "wb") as f:
                for start, end in ranges:
                    headers = {"Range": f"bytes={start}-{end}"}
                    rr = requests.get(grib_url, headers=headers, stream=True, timeout=60)
                    rr.raise_for_status()
                    for chunk in rr.iter_content(chunk_size=65536):
                        if chunk:
                            f.write(chunk)

            return f"‚úî [{fh_padded}] Gespeichert"

        except Exception as e:
            print(f"‚ùå [{fh_padded}] Fehler (Versuch {attempt}/{RETRY_LIMIT}): {e}")
            time.sleep(5 * attempt)

    return f"üí• [{fh_padded}] endg√ºltig fehlgeschlagen"


# Forecast-Stunden: 0‚Äì120 alle 1h, danach 3h-Raster
forecast_hours = list(range(0, 385, 6))

print(f"üöÄ Starte parallele AWS-Downloads ({MAX_WORKERS} Worker)‚Ä¶")

tasks = []
with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    for fh in forecast_hours:
        tasks.append(executor.submit(fetch_pmsl, fh))

    for future in as_completed(tasks):
        print(future.result())

print("\nüéâ COMPLETED: PMSL-Downloads abgeschlossen!")
